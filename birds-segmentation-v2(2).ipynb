{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5107085,"sourceType":"datasetVersion","datasetId":2700809},{"sourceId":121116660,"sourceType":"kernelVersion"}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Requirements env + moduls","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/ultralytics/yolov5  # clone\n!cd yolov5\n!pip install -r requirements.txt  # install\n!pip install ultralytics\n!pip install clearml\n!pip install pillow==11.0.0 \n!pip install requests==2.32.3 \n!pip install setuptools==75.6.0 \n!pip install tqdm==4.67.1\n!pip install -U ipywidgets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T10:03:21.317425Z","iopub.execute_input":"2024-12-31T10:03:21.317625Z","iopub.status.idle":"2024-12-31T10:04:48.515518Z","shell.execute_reply.started":"2024-12-31T10:03:21.317606Z","shell.execute_reply":"2024-12-31T10:04:48.514654Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'yolov5'...\nremote: Enumerating objects: 17080, done.\u001b[K\nremote: Counting objects: 100% (30/30), done.\u001b[K\nremote: Compressing objects: 100% (23/23), done.\u001b[K\nremote: Total 17080 (delta 21), reused 7 (delta 7), pack-reused 17050 (from 3)\u001b[K\nReceiving objects: 100% (17080/17080), 15.68 MiB | 30.53 MiB/s, done.\nInstalling collected packages: requests\n  Attempting uninstall: requests\n    Found existing installation: requests 2.31.0\n    Uninstalling requests-2.31.0:\n      Successfully uninstalled requests-2.31.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.8.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.2 which is incompatible.\ngcsfs 2023.12.2.post1 requires fsspec==2023.12.2, but you have fsspec 2024.2.0 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-pubsub 2.19.0 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.51.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed requests-2.32.3\nCollecting setuptools==75.6.0\n  Downloading setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\nDownloading setuptools-75.6.0-py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: setuptools\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 69.0.3\n    Uninstalling setuptools-69.0.3:\n      Successfully uninstalled setuptools-69.0.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed setuptools-75.6.0\nCollecting tqdm==4.67.1\n  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tqdm\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.66.1\n    Uninstalling tqdm-4.66.1:\n      Successfully uninstalled tqdm-4.66.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.8.1 requires keras-core, which is not installed.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.2 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed tqdm-4.67.1\nRequirement already satisfied: ipywidgets in /opt/conda/lib/python3.10/site-packages (7.7.1)\nCollecting ipywidgets\n  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (0.2.1)\nRequirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (8.20.0)\nRequirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (5.9.0)\nCollecting widgetsnbextension~=4.0.12 (from ipywidgets)\n  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\nCollecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.42)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\nDownloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.4/214.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n  Attempting uninstall: widgetsnbextension\n    Found existing installation: widgetsnbextension 3.6.6\n    Uninstalling widgetsnbextension-3.6.6:\n      Successfully uninstalled widgetsnbextension-3.6.6\n  Attempting uninstall: jupyterlab-widgets\n    Found existing installation: jupyterlab-widgets 3.0.9\n    Uninstalling jupyterlab-widgets-3.0.9:\n      Successfully uninstalled jupyterlab-widgets-3.0.9\n  Attempting uninstall: ipywidgets\n    Found existing installation: ipywidgets 7.7.1\n    Uninstalling ipywidgets-7.7.1:\n      Successfully uninstalled ipywidgets-7.7.1\nSuccessfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\nimport torch\nfrom IPython import display\nfrom IPython.display import clear_output\nfrom pathlib import Path\nimport yaml\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport glob\nimport io\nimport os\nimport cv2\nimport json\nimport shutil\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm\n%matplotlib inline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T10:04:48.519992Z","iopub.execute_input":"2024-12-31T10:04:48.520206Z","iopub.status.idle":"2024-12-31T10:04:52.955247Z","shell.execute_reply.started":"2024-12-31T10:04:48.520184Z","shell.execute_reply":"2024-12-31T10:04:52.954439Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"IMAGES_PATH = \"/kaggle/input/birdies/images\"\nLABELS_PATH = \"/kaggle/input/birdies/labels\"\n#NOTES_PATH = \"/kaggle/input/birdies/labels\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T10:06:16.891576Z","iopub.execute_input":"2024-12-31T10:06:16.891857Z","iopub.status.idle":"2024-12-31T10:06:16.895818Z","shell.execute_reply.started":"2024-12-31T10:06:16.891833Z","shell.execute_reply":"2024-12-31T10:06:16.894950Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"labels = os.listdir(LABELS_PATH)\n\n# Split data\ntrain, test = train_test_split(labels, test_size=0.2, shuffle=True)\nvalid, test = train_test_split(test, test_size=0.8)\n\nprint(f\"train: {len(train)}; test: {len(test)}; valid: {len(valid)}\") #valid: {len(valid)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T10:06:19.134133Z","iopub.execute_input":"2024-12-31T10:06:19.134976Z","iopub.status.idle":"2024-12-31T10:06:19.209721Z","shell.execute_reply.started":"2024-12-31T10:06:19.134923Z","shell.execute_reply":"2024-12-31T10:06:19.208853Z"}},"outputs":[{"name":"stdout","text":"train: 1608; test: 322; valid: 80\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def move_files_to_dir(files, dirname):\n    for label_filename in tqdm(files, total = len(files)):\n        image_filename = f\"{label_filename[:-4]}.jpg\"\n        shutil.copy(f\"{IMAGES_PATH}/{image_filename}\", f\"{dirname}/images/{image_filename}\")\n        shutil.copy(f\"{LABELS_PATH}/{label_filename}\", f\"{dirname}/labels/{label_filename}\")\n\n\n# Make folders\nif not os.path.exists(\"test/images\"):\n    os.makedirs(\"test/images\")\nif not os.path.exists(\"test/labels\"):\n    os.makedirs(\"test/labels\")\nif not os.path.exists(\"train/images\"):\n    os.makedirs(\"train/images\")\nif not os.path.exists(\"train/labels\"):\n    os.makedirs(\"train/labels\")\nif not os.path.exists(\"valid/images\"):\n    os.makedirs(\"valid/images\")\nif not os.path.exists(\"valid/labels\"):\n    os.makedirs(\"valid/labels\")\n\n# Move splits to folders\nmove_files_to_dir(train, \"train\")\nmove_files_to_dir(test, \"test\")\nmove_files_to_dir(valid, \"valid\")\n\n# Path\ntrain_path = \"../train/images\"\ntest_path = \"../test/images\"\nvalid_path = \"../valid/images\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T10:06:23.244718Z","iopub.execute_input":"2024-12-31T10:06:23.245056Z","iopub.status.idle":"2024-12-31T10:07:02.559138Z","shell.execute_reply.started":"2024-12-31T10:06:23.245028Z","shell.execute_reply":"2024-12-31T10:07:02.558354Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 1608/1608 [00:32<00:00, 50.21it/s]\n100%|██████████| 322/322 [00:05<00:00, 57.30it/s]\n100%|██████████| 80/80 [00:01<00:00, 48.30it/s]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Make Yaml","metadata":{}},{"cell_type":"code","source":"with open(\"data.yaml\", \"w\") as file:\n    yaml.dump({\n        \"train\": train_path,\n        \"test\": test_path,\n        \"val\": valid_path,\n        \"names\": ['birds'],\n        \"nc\": 1,\n        #\"names\": [f'{name}' for name in names]\n    } , stream=file, default_flow_style=None)\n\nprint(\"Now we are ready to train yolov5 model\")\n! ls ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T10:07:08.855630Z","iopub.execute_input":"2024-12-31T10:07:08.856275Z","iopub.status.idle":"2024-12-31T10:07:09.884326Z","shell.execute_reply.started":"2024-12-31T10:07:08.856227Z","shell.execute_reply":"2024-12-31T10:07:09.883357Z"}},"outputs":[{"name":"stdout","text":"Now we are ready to train yolov5 model\ndata.yaml  test  train\tvalid  yolov5\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"PROJECT_NAME = \"birds\"\nBASE_MODEL = \"yolov5m6.pt\"\nBATCH = 64\nTRAIN_EPOCHS = 50\n#VAL_BATCH = 64","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T10:09:57.190362Z","iopub.execute_input":"2024-12-31T10:09:57.190761Z","iopub.status.idle":"2024-12-31T10:09:57.211637Z","shell.execute_reply.started":"2024-12-31T10:09:57.190731Z","shell.execute_reply":"2024-12-31T10:09:57.210460Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"! python yolov5/train.py --batch $BATCH --img 640 --epochs $TRAIN_EPOCHS --data \"data.yaml\" --weights $BASE_MODEL --project $PROJECT_NAME --name 'feature_extraction' --cache --freeze 12","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T10:10:02.427805Z","iopub.execute_input":"2024-12-31T10:10:02.428678Z"}},"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: WARNING ⚠️ wandb is deprecated and will be removed in a future release. See supported integrations at https://github.com/ultralytics/yolov5#integrations.\n2024-12-31 10:10:11.009386: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-12-31 10:10:11.009452: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-12-31 10:10:11.011151: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) \n\u001b[34m\u001b[1mwandb\u001b[0m: W&B disabled due to login timeout.\n\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5m6.pt, cfg=, data=data.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=64, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=yolov5/data/hyps, resume_evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=birds, name=feature_extraction, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[12], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['thop>=0.1.1'] not found, attempting AutoUpdate...\nCollecting thop>=0.1.1\n  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from thop>=0.1.1) (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->thop>=0.1.1) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->thop>=0.1.1) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->thop>=0.1.1) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->thop>=0.1.1) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->thop>=0.1.1) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->thop>=0.1.1) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->thop>=0.1.1) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->thop>=0.1.1) (1.3.0)\nDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\nInstalling collected packages: thop\nSuccessfully installed thop-0.1.1.post2209072238\n\n\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 7.9s, installed 1 package: ['thop>=0.1.1']\n\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n\nYOLOv5 🚀 v7.0-390-g0797106a Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla T4, 15095MiB)\n\n\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir birds', view at http://localhost:6006/\n\u001b[34m\u001b[1mClearML: \u001b[0mWARNING ⚠️ ClearML is installed but not configured, skipping ClearML logging. See https://docs.ultralytics.com/yolov5/tutorials/clearml_logging_integration#readme\nDownloading https://github.com/ultralytics/assets/releases/download/v0.0.0/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n100%|████████████████████████████████████████| 755k/755k [00:00<00:00, 17.2MB/s]\nDownloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m6.pt to yolov5m6.pt...\n100%|███████████████████████████████████████| 69.0M/69.0M [00:00<00:00, 227MB/s]\n\nOverriding model.yaml nc=80 with nc=1\n\n                 from  n    params  module                                  arguments                     \n  0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]              \n  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n  2                -1  2     65280  models.common.C3                        [96, 96, 2]                   \n  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n  4                -1  4    444672  models.common.C3                        [192, 192, 4]                 \n  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n  6                -1  6   2512896  models.common.C3                        [384, 384, 6]                 \n  7                -1  1   1991808  models.common.Conv                      [384, 576, 3, 2]              \n  8                -1  2   2327040  models.common.C3                        [576, 576, 2]                 \n  9                -1  1   3982848  models.common.Conv                      [576, 768, 3, 2]              \n 10                -1  2   4134912  models.common.C3                        [768, 768, 2]                 \n 11                -1  1   1476864  models.common.SPPF                      [768, 768, 5]                 \n 12                -1  1    443520  models.common.Conv                      [768, 576, 1, 1]              \n 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 14           [-1, 8]  1         0  models.common.Concat                    [1]                           \n 15                -1  2   2658816  models.common.C3                        [1152, 576, 2, False]         \n 16                -1  1    221952  models.common.Conv                      [576, 384, 1, 1]              \n 17                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 18           [-1, 6]  1         0  models.common.Concat                    [1]                           \n 19                -1  2   1182720  models.common.C3                        [768, 384, 2, False]          \n 20                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n 21                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 22           [-1, 4]  1         0  models.common.Concat                    [1]                           \n 23                -1  2    296448  models.common.C3                        [384, 192, 2, False]          \n 24                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n 25          [-1, 20]  1         0  models.common.Concat                    [1]                           \n 26                -1  2   1035264  models.common.C3                        [384, 384, 2, False]          \n 27                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n 28          [-1, 16]  1         0  models.common.Concat                    [1]                           \n 29                -1  2   2437632  models.common.C3                        [768, 576, 2, False]          \n 30                -1  1   2987136  models.common.Conv                      [576, 576, 3, 2]              \n 31          [-1, 12]  1         0  models.common.Concat                    [1]                           \n 32                -1  2   4429824  models.common.C3                        [1152, 768, 2, False]         \n 33  [23, 26, 29, 32]  1     34632  models.yolo.Detect                      [1, [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], [192, 384, 576, 768]]\nModel summary: 379 layers, 35275944 parameters, 35275944 gradients, 49.3 GFLOPs\n\nTransferred 619/627 items from yolov5m6.pt\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\nfreezing model.0.conv.weight\nfreezing model.0.bn.weight\nfreezing model.0.bn.bias\nfreezing model.1.conv.weight\nfreezing model.1.bn.weight\nfreezing model.1.bn.bias\nfreezing model.2.cv1.conv.weight\nfreezing model.2.cv1.bn.weight\nfreezing model.2.cv1.bn.bias\nfreezing model.2.cv2.conv.weight\nfreezing model.2.cv2.bn.weight\nfreezing model.2.cv2.bn.bias\nfreezing model.2.cv3.conv.weight\nfreezing model.2.cv3.bn.weight\nfreezing model.2.cv3.bn.bias\nfreezing model.2.m.0.cv1.conv.weight\nfreezing model.2.m.0.cv1.bn.weight\nfreezing model.2.m.0.cv1.bn.bias\nfreezing model.2.m.0.cv2.conv.weight\nfreezing model.2.m.0.cv2.bn.weight\nfreezing model.2.m.0.cv2.bn.bias\nfreezing model.2.m.1.cv1.conv.weight\nfreezing model.2.m.1.cv1.bn.weight\nfreezing model.2.m.1.cv1.bn.bias\nfreezing model.2.m.1.cv2.conv.weight\nfreezing model.2.m.1.cv2.bn.weight\nfreezing model.2.m.1.cv2.bn.bias\nfreezing model.3.conv.weight\nfreezing model.3.bn.weight\nfreezing model.3.bn.bias\nfreezing model.4.cv1.conv.weight\nfreezing model.4.cv1.bn.weight\nfreezing model.4.cv1.bn.bias\nfreezing model.4.cv2.conv.weight\nfreezing model.4.cv2.bn.weight\nfreezing model.4.cv2.bn.bias\nfreezing model.4.cv3.conv.weight\nfreezing model.4.cv3.bn.weight\nfreezing model.4.cv3.bn.bias\nfreezing model.4.m.0.cv1.conv.weight\nfreezing model.4.m.0.cv1.bn.weight\nfreezing model.4.m.0.cv1.bn.bias\nfreezing model.4.m.0.cv2.conv.weight\nfreezing model.4.m.0.cv2.bn.weight\nfreezing model.4.m.0.cv2.bn.bias\nfreezing model.4.m.1.cv1.conv.weight\nfreezing model.4.m.1.cv1.bn.weight\nfreezing model.4.m.1.cv1.bn.bias\nfreezing model.4.m.1.cv2.conv.weight\nfreezing model.4.m.1.cv2.bn.weight\nfreezing model.4.m.1.cv2.bn.bias\nfreezing model.4.m.2.cv1.conv.weight\nfreezing model.4.m.2.cv1.bn.weight\nfreezing model.4.m.2.cv1.bn.bias\nfreezing model.4.m.2.cv2.conv.weight\nfreezing model.4.m.2.cv2.bn.weight\nfreezing model.4.m.2.cv2.bn.bias\nfreezing model.4.m.3.cv1.conv.weight\nfreezing model.4.m.3.cv1.bn.weight\nfreezing model.4.m.3.cv1.bn.bias\nfreezing model.4.m.3.cv2.conv.weight\nfreezing model.4.m.3.cv2.bn.weight\nfreezing model.4.m.3.cv2.bn.bias\nfreezing model.5.conv.weight\nfreezing model.5.bn.weight\nfreezing model.5.bn.bias\nfreezing model.6.cv1.conv.weight\nfreezing model.6.cv1.bn.weight\nfreezing model.6.cv1.bn.bias\nfreezing model.6.cv2.conv.weight\nfreezing model.6.cv2.bn.weight\nfreezing model.6.cv2.bn.bias\nfreezing model.6.cv3.conv.weight\nfreezing model.6.cv3.bn.weight\nfreezing model.6.cv3.bn.bias\nfreezing model.6.m.0.cv1.conv.weight\nfreezing model.6.m.0.cv1.bn.weight\nfreezing model.6.m.0.cv1.bn.bias\nfreezing model.6.m.0.cv2.conv.weight\nfreezing model.6.m.0.cv2.bn.weight\nfreezing model.6.m.0.cv2.bn.bias\nfreezing model.6.m.1.cv1.conv.weight\nfreezing model.6.m.1.cv1.bn.weight\nfreezing model.6.m.1.cv1.bn.bias\nfreezing model.6.m.1.cv2.conv.weight\nfreezing model.6.m.1.cv2.bn.weight\nfreezing model.6.m.1.cv2.bn.bias\nfreezing model.6.m.2.cv1.conv.weight\nfreezing model.6.m.2.cv1.bn.weight\nfreezing model.6.m.2.cv1.bn.bias\nfreezing model.6.m.2.cv2.conv.weight\nfreezing model.6.m.2.cv2.bn.weight\nfreezing model.6.m.2.cv2.bn.bias\nfreezing model.6.m.3.cv1.conv.weight\nfreezing model.6.m.3.cv1.bn.weight\nfreezing model.6.m.3.cv1.bn.bias\nfreezing model.6.m.3.cv2.conv.weight\nfreezing model.6.m.3.cv2.bn.weight\nfreezing model.6.m.3.cv2.bn.bias\nfreezing model.6.m.4.cv1.conv.weight\nfreezing model.6.m.4.cv1.bn.weight\nfreezing model.6.m.4.cv1.bn.bias\nfreezing model.6.m.4.cv2.conv.weight\nfreezing model.6.m.4.cv2.bn.weight\nfreezing model.6.m.4.cv2.bn.bias\nfreezing model.6.m.5.cv1.conv.weight\nfreezing model.6.m.5.cv1.bn.weight\nfreezing model.6.m.5.cv1.bn.bias\nfreezing model.6.m.5.cv2.conv.weight\nfreezing model.6.m.5.cv2.bn.weight\nfreezing model.6.m.5.cv2.bn.bias\nfreezing model.7.conv.weight\nfreezing model.7.bn.weight\nfreezing model.7.bn.bias\nfreezing model.8.cv1.conv.weight\nfreezing model.8.cv1.bn.weight\nfreezing model.8.cv1.bn.bias\nfreezing model.8.cv2.conv.weight\nfreezing model.8.cv2.bn.weight\nfreezing model.8.cv2.bn.bias\nfreezing model.8.cv3.conv.weight\nfreezing model.8.cv3.bn.weight\nfreezing model.8.cv3.bn.bias\nfreezing model.8.m.0.cv1.conv.weight\nfreezing model.8.m.0.cv1.bn.weight\nfreezing model.8.m.0.cv1.bn.bias\nfreezing model.8.m.0.cv2.conv.weight\nfreezing model.8.m.0.cv2.bn.weight\nfreezing model.8.m.0.cv2.bn.bias\nfreezing model.8.m.1.cv1.conv.weight\nfreezing model.8.m.1.cv1.bn.weight\nfreezing model.8.m.1.cv1.bn.bias\nfreezing model.8.m.1.cv2.conv.weight\nfreezing model.8.m.1.cv2.bn.weight\nfreezing model.8.m.1.cv2.bn.bias\nfreezing model.9.conv.weight\nfreezing model.9.bn.weight\nfreezing model.9.bn.bias\nfreezing model.10.cv1.conv.weight\nfreezing model.10.cv1.bn.weight\nfreezing model.10.cv1.bn.bias\nfreezing model.10.cv2.conv.weight\nfreezing model.10.cv2.bn.weight\nfreezing model.10.cv2.bn.bias\nfreezing model.10.cv3.conv.weight\nfreezing model.10.cv3.bn.weight\nfreezing model.10.cv3.bn.bias\nfreezing model.10.m.0.cv1.conv.weight\nfreezing model.10.m.0.cv1.bn.weight\nfreezing model.10.m.0.cv1.bn.bias\nfreezing model.10.m.0.cv2.conv.weight\nfreezing model.10.m.0.cv2.bn.weight\nfreezing model.10.m.0.cv2.bn.bias\nfreezing model.10.m.1.cv1.conv.weight\nfreezing model.10.m.1.cv1.bn.weight\nfreezing model.10.m.1.cv1.bn.bias\nfreezing model.10.m.1.cv2.conv.weight\nfreezing model.10.m.1.cv2.bn.weight\nfreezing model.10.m.1.cv2.bn.bias\nfreezing model.11.cv1.conv.weight\nfreezing model.11.cv1.bn.weight\nfreezing model.11.cv1.bn.bias\nfreezing model.11.cv2.conv.weight\nfreezing model.11.cv2.bn.weight\nfreezing model.11.cv2.bn.bias\n\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 103 weight(decay=0.0), 107 weight(decay=0.0005), 107 bias\nWARNING ⚠️ DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\nSee Multi-GPU Tutorial at https://docs.ultralytics.com/yolov5/tutorials/multi_gpu_training to get started.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/train/labels... 1608 images, 0 backgrounds, 0 co\u001b[0m\n\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/train/labels.cache\n\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (1.4GB ram): 100%|██████████| 1608/1608 [00:14<00:00, 108.\u001b[0m\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/valid/labels... 80 images, 0 backgrounds, 0 corrup\u001b[0m\n\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/valid/labels.cache\n\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram): 100%|██████████| 80/80 [00:01<00:00, 57.99it/s]\u001b[0m\n\n\u001b[34m\u001b[1mAutoAnchor: \u001b[0m7.15 anchors/target, 0.996 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\nPlotting labels to birds/feature_extraction/labels.jpg... \n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\nImage sizes 640 train, 640 val\nUsing 2 dataloader workers\nLogging results to \u001b[1mbirds/feature_extraction\u001b[0m\nStarting training for 50 epochs...\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       0/49      4.67G    0.07813    0.02786          0         16        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all         80         80      0.649      0.863      0.704      0.328\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       1/49         8G    0.04441    0.02259          0         25        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all         80         80      0.736      0.766      0.866      0.486\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       2/49         8G    0.03966    0.01899          0         24        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all         80         80      0.667      0.752      0.812      0.444\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       3/49         8G     0.0354    0.01634          0         19        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all         80         80      0.945      0.988      0.977      0.557\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       4/49         8G    0.03115    0.01459          0         22        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all         80         80       0.63       0.95      0.746      0.422\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       5/49         8G     0.0279    0.01398          0         23        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all         80         80      0.972          1      0.993       0.65\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       6/49         8G    0.02616    0.01368          0         18        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all         80         80      0.975      0.993      0.991      0.742\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       7/49         8G    0.02384    0.01293          0         21        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all         80         80      0.972          1      0.995      0.823\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       8/49         8G    0.02226    0.01329          0         25        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all         80         80      0.977      0.988      0.995      0.798\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       9/49         8G    0.02145    0.01268          0         25        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all         80         80      0.963          1      0.994      0.756\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      10/49         8G    0.02137    0.01225          0        173        640:  ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wildcard = f\"{PROJECT_NAME}/validation_on_test_data*\"\n! rm -r $wildcard","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"WEIGHTS_BEST = f\"{PROJECT_NAME}/feature_extraction/weights/best.pt\"\n! python yolov5/val.py --weights $WEIGHTS_BEST --batch $VAL_BATCH --data 'data.yaml' --task test --project $PROJECT_NAME --name 'validation_on_test_data' --augment","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Delete old results\nwildcard = f\"{PROJECT_NAME}/detect_test*\"\n! rm -r $wildcard","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! python yolov5/detect.py --weights $WEIGHTS_BEST --conf 0.6 --source 'test/images' --project $PROJECT_NAME --name 'detect_test' --augment --line=3","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def read_images(dirpath):\n  images = []\n  for img_filename in os.listdir(dirpath):\n    images.append(mpimg.imread(f\"{dirpath}/{img_filename}\"))\n  return images\n\ndef label_test_images(test_images_path, test_labels_path, classes):\n  test_images = os.listdir(test_images_path)\n  labeled_images = []\n\n  for idx, test_image_filename in enumerate(test_images):\n    image = mpimg.imread(f\"{test_images_path}/{test_image_filename}\")\n    \n    x_shape, y_shape = image.shape[1], image.shape[0]\n\n    test_label_filename = f\"{test_image_filename[:-4]}.txt\"\n    \n    with open(f\"{test_labels_path}/{test_label_filename}\", \"r\") as f:\n      lines = f.readlines()\n\n      for line in lines:\n        # Parse line\n        box = line.split()\n        class_idx = box[0]\n        \n        class_name = names[int(class_idx)]\n        x_center, y_center, box_w, box_h = int(float(box[1])*x_shape), int(float(box[2])*y_shape), int(float(box[3])*x_shape), int(float(box[3])*y_shape)\n        x1, y1, x2, y2 = x_center-int(box_w/2), y_center-int(box_h/2), x_center+int(box_w/2), y_center+int(box_h/2)\n\n        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), 3)\n        cv2.putText(image, class_name, (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 3)\n\n    labeled_images.append(image)\n\n  return labeled_images","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"detect_path = f\"{PROJECT_NAME}/detect_test\"\ntest_images_path = f\"test/images\"\ntest_labels_path = f\"test/labels\"\n\ndetected_images = read_images(detect_path)\ntest_labeled_images = label_test_images(test_images_path, test_labels_path, classes=names)\n\nstacked_images = [np.hstack([detected_images[idx], test_labeled_images[idx]]) for idx in range(len(detected_images))]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for image in stacked_images:\n  fig = plt.figure(figsize=(40, 15))\n  ax1 = fig.add_subplot(2,2,1)\n  ax1.imshow(image)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PROJECT_FOLDER -> feature_extraction (your best) -> weights -> best.pt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"/kaggle/working/yolov5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"yolo = YOLO()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = YOLO(\"yolo11n-obb.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"data.yaml\", epochs=2, imgsz=224)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_size = (224, 224, 3) \nbach_size = 128 \nimg_dir = r'/kaggle/input/birdies/images'\nlabel_dir = r'/kaggle/input/birdies/labels'\nworking_dir = r'/kaggle/working/'\ntest_dir = r'/kaggle/input/birdies/test images'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_box(label_dir, img_size): \\\n    # оптимизировано для диапазона чисел не более 2^16\n    with open(label_dir) as f: # открытие файлов в ириктории\n        line=f.readline()    \n    box_params = line.split() # разделяем числа по пробелу\n    xc = np.float16(box_params[1]) * img_size[1] # центр картинки по x\n    yc = np.float16(box_params[2]) * img_size[0]   # центр картинки по y \n    \n    bw = np.float16(box_params[3]) * img_size[1] # ширина\n    bh = np.float16(box_params[4]) * img_size[0] # высота\n\n    # отрисовка линий ящика\n    x_min = np.int16((xc - bw/2)) # отложим от центра половину ширины вправо и влево\n    \n    x_max = np.int16((xc + bw/2))\n    y_min = np.int16((yc - bh/2)) # отложим от центра половину длины вправо и влево\n    \n    y_max = np.int16((yc + bh/2))\n    \n    return x_min, y_min, x_max, y_max","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ","metadata":{"execution":{"iopub.status.busy":"2024-03-19T09:13:13.865446Z","iopub.execute_input":"2024-03-19T09:13:13.865888Z","iopub.status.idle":"2024-03-19T09:13:13.878003Z","shell.execute_reply.started":"2024-03-19T09:13:13.865857Z","shell.execute_reply":"2024-03-19T09:13:13.876828Z"}}},{"cell_type":"code","source":"get_box(label_dir + '/0010.txt', img_size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Отрисуем ящики на картинках","metadata":{}},{"cell_type":"code","source":"# Подготовка путей и данных\nimg_list = np.sort(os.listdir(img_dir))\nlabel_list = np.sort(os.listdir(label_dir))\nzip_list = zip(img_list, label_list) # генератор\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\ni=0\nfor img, box in zip_list:\n    img_path = os.path.join(img_dir, img) # соеденяем в путь к конкретной картинки\n    label_path = os.path.join(label_dir, box) # получаем путь ящика\n    img_draw = cv2.imread(img_path)\n    img_draw = cv2.resize(img_draw, img_size[:2])\n    xmin, ymin, xmax, ymax = get_box(label_path, img_size)      \n        \n    cv2.rectangle(img_draw, (xmin, ymin), (xmax, ymax), (255,0,0), 6) # рисуем прямоугольник на картинки\n    plt.subplot(5,4, i+1)\n    plt.axis('off')\n    plt.imshow(img_draw)\n    i += 1\n \n    if i >= 20:\n        break\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%time\ndef make_data(img_list, label_list, img_dir,label_dir, img_size):\n    zip_list = zip(img_list, label_list)\n    boxes = []\n    images = []\n    i = 1\n    count = len(img_list)\n    for img, label in zip_list:\n        msg = f'processing image {i} of  {count}'\n        i += 1\n        print(msg, '\\r', end='')\n        img_path = os.path.join(img_dir, img)\n        labelpath = os.path.join(label_dir, label)\n        img_read = cv2.imread(img_path)\n        img_read = cv2.resize(img_read, img_size[:2])\n        xmin, ymin, xmax, ymax= get_box(label_path, img_size)  \n        box = [xmin, ymin, xmax, ymax] \n        \n        images.append(img_read)\n        boxes.append(box)\n    \n    images = np.array(images)\n    boxes =  np.array(boxes)\n    return images, boxes\n\nimages, boxes = make_data(img_list, label_list, img_dir,label_dir, img_size)\nX_train, X_test, y_train, y_test = train_test_split(images, boxes, train_size=.7, shuffle=True, random_state=142)  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## EfficientNetB1","metadata":{}},{"cell_type":"code","source":"from ultralytics import YOLO\n \n# Load the model.\nmodel = YOLO('yolov8n.pt')\n \n# Training.\nresults = model.train(\n   data='custom_data.yaml',\n   imgsz=640,\n   epochs=10,\n   batch=8,\n   name='yolov8n_custom')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lr = .0005\n\nbase = tf.keras.applications.efficientnet.EfficientNetB1(\n    include_top=False, weights=\"imagenet\",input_shape=img_size, pooling='avg')\n\nbase.trainable = True\nx = base.output\n\nx = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\nx = Dense(256, kernel_regularizer = regularizers.l2(0.016), activity_regularizer=regularizers.l1(0.006),\n                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\nx = Dropout(rate=0.33, seed=123)(x)       \noutput = Dense(4, activation='linear')(x)\n\n\nmodel = Model(inputs = base.input, outputs = output)\n\nmodel.compile(RMSprop(learning_rate=lr), loss='mse', metrics=['accuracy']) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}